{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f812e0-03a5-4321-9792-0006eb610f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from PIL import Image\n",
    "from math import exp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from CBD.utils.resnet import resnet20\n",
    "from CBD.utils.vgg import vgg16\n",
    "from CBD.utils.mobilenetv2 import mobilenetv2\n",
    "from GuidedDiffusionPur.guided_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    args_to_dict,\n",
    ")\n",
    "from GuidedDiffusionPur.pytorch_diffusion.diffusion import Diffusion\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f92319-5e68-4d47-b2e6-ad4b9d5f5d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root='../../../pk-data-4T/imagenette/train', \n",
    "                            transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                                ]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "test_dataset = ImageFolder(root='../../../pk-data-4T/imagenette/val', \n",
    "                            transform=transforms.Compose([\n",
    "                                    transforms.CenterCrop(256),\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                            ]))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343da8c7-5a45-47c8-9c27-379c2a33c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet20().to(device)\n",
    "# model = vgg16().to(device)\n",
    "# model = mobilenetv2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1cc09-911b-47e8-bcdb-cefe44d78279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=torch.tensor([100, 150]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b7b12-288a-47c9-b68e-7afcb3fbc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for (images, labels) in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            accuracy = 100 * correct / total\n",
    "    print('Epoch:%d, Accuracy on the test set: %.1f %%' % (epoch, accuracy))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), './FBA/model/imagenette_resnet20.pth')\n",
    "        # torch.save(model.state_dict(), './FBA/model/imagenette_vgg16.pth')\n",
    "        # torch.save(model.state_dict(), './FBA/model/imagenette_mobilenetv2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61e40e-7f46-4b93-a0a7-d62f94efdcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config ={\n",
    "'attention_resolutions': '32,16,8', \n",
    " 'batch_size': 30, \n",
    " 'channel_mult': '', \n",
    " 'class_cond': False, \n",
    " 'clip_denoised': True, \n",
    " 'diffusion_steps': 1000, \n",
    " 'dropout': 0.0, \n",
    " 'image_size': 256, \n",
    " 'learn_sigma': True, \n",
    " 'noise_schedule': 'linear', \n",
    " 'num_channels': 256, \n",
    " 'num_head_channels': 64, \n",
    " 'num_heads': 4, \n",
    " 'num_heads_upsample': -1, \n",
    " 'num_res_blocks': 2, \n",
    " 'num_samples': 10000, \n",
    " 'predict_xstart': False, \n",
    " 'resblock_updown': True, \n",
    " 'rescale_learned_sigmas': False, \n",
    " 'rescale_timesteps': False, \n",
    " 'timestep_respacing': '250', \n",
    " 'use_checkpoint': False, \n",
    " 'use_ddim': False, \n",
    " 'use_fp16': False, \n",
    " 'use_kl': False, \n",
    " 'use_new_attention_order': False, \n",
    " 'use_scale_shift_norm': True\n",
    "}\n",
    "\n",
    "new_config = argparse.Namespace()\n",
    "for key, value in config.items():\n",
    "    setattr(new_config, key, value)\n",
    "    \n",
    "sampler, diffusion = create_model_and_diffusion(**args_to_dict(new_config, model_and_diffusion_defaults().keys()))\n",
    "sampler.load_state_dict(torch.load('./GuidedDiffusionPur/models/256x256_diffusion_uncond.pt', map_location='cpu'))\n",
    "sampler.eval().to(device)\n",
    "\n",
    "transform_normalize = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "def sample_imagenet(inputs, t_steps, max_iter, diffusion, use_ddim, sampler, clip_denoised, cond, guide_mode, model, device):\n",
    "    t_steps = (torch.ones(inputs.shape[0], device=device).long())* (t_steps-1)\n",
    "    shape = list(inputs.shape)\n",
    "    model_kwargs = {}\n",
    "\n",
    "    def cond_fn(inputs_reverse_t, t):\n",
    "        with torch.enable_grad():\n",
    "            inputs_in = inputs_reverse_t.detach().requires_grad_(True)\n",
    "            inputs_t = inputs\n",
    "            # inputs_t = diffusion.q_sample(inputs, t)\n",
    "            if guide_mode == 'MSE': \n",
    "                selected = -1 * F.mse_loss(inputs_in, inputs_t)\n",
    "                scale = diffusion.compute_scale(inputs_in, t, 8/255. / 3. / 1000)\n",
    "            elif guide_mode == 'SSIM':\n",
    "                selected = pytorch_ssim.ssim(inputs_in, inputs_t)\n",
    "                scale = diffusion.compute_scale(inputs_in, t, 8/255. / 3. / 1000)\n",
    "            elif guide_mode == 'LPIPS':\n",
    "                _, feature_21, feature_31, feature_41 = model.forward(transform_normalize(inputs_in), return_hidden=False, return_activation=True)\n",
    "                _, feature_22, feature_32, feature_42 = model.forward(transform_normalize(inputs_t), return_hidden=False, return_activation=True)\n",
    "                \n",
    "                min_value = torch.min(feature_22).item()\n",
    "                max_value = torch.max(feature_22).item()\n",
    "                median_value = (min_value + max_value)/2.0\n",
    "                \n",
    "                feature_22[feature_22 > median_value] += (median_value/10.0)\n",
    "                feature_22[feature_22 < median_value] -= (median_value/10.0)\n",
    "                feature_22 = torch.clamp(feature_22, min_value, max_value)\n",
    "                                \n",
    "                selected = -1.0*torch.mean((feature_21 - feature_22)**2)\n",
    "                \n",
    "                scale = diffusion.compute_scale(inputs_in,t, 8/255. / 3. / 2000)\n",
    "            elif guide_mode == 'CONSTANT': \n",
    "                selected = pytorch_ssim.ssim(inputs_in, inputs_t)\n",
    "                scale = 1000\n",
    "            return torch.autograd.grad(selected.sum(), inputs_in)[0] * scale\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs_t_reverse = inputs\n",
    "        for i in range(max_iter):            \n",
    "            noises = diffusion.q_sample(inputs_t_reverse, t_steps)\n",
    "            sample_fn = diffusion.p_sample_loop if not use_ddim else diffusion.ddim_sample_loop\n",
    "            inputs_t_reverse = sample_fn(\n",
    "                    sampler,\n",
    "                    shape,\n",
    "                    t_steps = t_steps,\n",
    "                    noise = noises,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    cond_fn = cond_fn if cond else None,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                )\n",
    "        outputs = inputs_t_reverse.clone().detach()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723add7a-c031-4694-b10a-a5a342de7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root='../../../pk-data-4T/imagenette_poison/train', \n",
    "                            transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                ]))\n",
    "\n",
    "test_dataset = ImageFolder(root='../../../pk-data-4T/imagenette_poison/val', \n",
    "                            transform=transforms.Compose([\n",
    "                                    transforms.CenterCrop(256),\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b07c6-89ed-4ed4-92f7-fffd167017b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = '../../../pk-data-4T/imagenette_poison/train_poison/0'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    image, label = train_dataset[i]\n",
    "    output = sample_imagenet(inputs=image.unsqueeze(0).to(device), \n",
    "                                t_steps=20, \n",
    "                                max_iter=1, \n",
    "                                diffusion=diffusion, \n",
    "                                use_ddim=True, \n",
    "                                sampler=sampler, \n",
    "                                clip_denoised=True, \n",
    "                                cond=True, \n",
    "                                guide_mode='LPIPS',\n",
    "                                model = model.eval().to(device),\n",
    "                                device=device)\n",
    "    image_pil = transforms.ToPILImage()(output[0].cpu())\n",
    "    image_pil.save(os.path.join(output_dir, f'imagenette_train_poison_image_{i}.png'))\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500c4c8-d28f-42e1-b818-6f647b94f4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = '../../../pk-data-4T/imagenette_poison/val_poison'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    image, label = test_dataset[i]\n",
    "    output = sample_imagenet(inputs=image.unsqueeze(0).to(device), \n",
    "                                t_steps=20, \n",
    "                                max_iter=1, \n",
    "                                diffusion=diffusion, \n",
    "                                use_ddim=True, \n",
    "                                sampler=sampler, \n",
    "                                clip_denoised=True, \n",
    "                                cond=True, \n",
    "                                guide_mode='LPIPS',\n",
    "                                model = model.eval().to(device),\n",
    "                                device=device)    \n",
    "    class_dir = os.path.join(output_dir, str(label))\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    image_pil = transforms.ToPILImage()(output[0].cpu())\n",
    "    image_pil.save(os.path.join(class_dir, f'imagenette_val_poison_image_{i}.png'))\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f844a6-1556-4006-80cc-405cd5e33ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root='../../../pk-data-4T/ruiyang/imagenette_poison/train_poison', \n",
    "                            transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                                ]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "test_dataset = ImageFolder(root='../../../pk-data-4T/ruiyang/imagenette_poison/val', \n",
    "                            transform=transforms.Compose([\n",
    "                                    transforms.CenterCrop(256),\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                            ]))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "test_dataset_poison = ImageFolder(root='../../../pk-data-4T/ruiyang/imagenette_poison/val_poison', \n",
    "                            transform=transforms.Compose([\n",
    "                                    transforms.CenterCrop(256),\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                            ]))\n",
    "\n",
    "test_loader_poison = DataLoader(test_dataset_poison, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dd534-0647-4799-8a7e-41c3757cfb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = resnet20()\n",
    "# model.load_state_dict(torch.load('./FBA/model/imagenette_resnet20.pth', map_location='cpu'))\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=torch.tensor([100, 150]).tolist())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for (images, labels) in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            accuracy = 100 * correct / total\n",
    "    print('Epoch:%d, Clean Accuracy on the test set: %.1f %%' % (epoch, accuracy))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (images, _) in test_loader_poison:\n",
    "            labels = torch.full((images.shape[0],), 0, dtype=torch.long)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            accuracy = 100 * correct / total\n",
    "    print('Epoch:%d, Poisoned Accuracy on the test set: %.1f %%' % (epoch, accuracy))\n",
    "    torch.save(model.state_dict(), '../../../pk-data-4T/ruiyang/model/imagenette_resnet20_poison_{}.pth'.format(epoch))\n",
    "    # if accuracy > best_accuracy:\n",
    "    #     best_accuracy = accuracy\n",
    "    #     torch.save(model.state_dict(), './FBA/model/imagenette_resnet20_poison.pth')\n",
    "        # torch.save(model.state_dict(), './FBA/model/imagenette_vgg16_poison.pth')\n",
    "        # torch.save(model.state_dict(), './FBA/model/imagenette_mobilenetv2_poison.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
